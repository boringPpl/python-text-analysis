{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Text Analysis\n",
    "\n",
    "## Pre-introduction\n",
    "\n",
    "We'll be spending a lot of time today manipulating text. Make sure you remember how to split, join, and search strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Introduction\n",
    "\n",
    "We've spent a lot of time in python dealing with text data, and that's because text data is everywhere. It is the primary form of communication between persons and persons, persons and computers, and computers and computers. The kind of inferential methods that we apply to text data, however, are different from those applied to tabular data. \n",
    "\n",
    "This is partly because documents are typically specified in a way that expresses both structure and content using text (i.e. the document object model).\n",
    "\n",
    "Largely, however, it's because text is difficult to turn into numbers in a way that preserves the information in the document. Today, we'll talk about dominant language model in NLP and the basics of how to implement it in Python.\n",
    "\n",
    "### The term-document model\n",
    "\n",
    "This is also sometimes referred to as \"bag-of-words\" by those who don't think very highly of it. The term document model looks at language as individual communicative efforts that contain one or more tokens. The kind and number of the tokens in a document tells you something about what is attempting to be communicated, and the order of those tokens is ignored.\n",
    "\n",
    "To start with, let's load a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('webtext')\n",
    "document = nltk.corpus.webtext.open('grail.txt').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what's in this document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(document[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we've gotten ourselves a bit of the script from Monty Python and the Holy Grail. Note that when we are looking at the text, part of the structure of the document is written in tokens. For example, stage directions have been placed in brackets, and the names of the person speaking are in all caps.\n",
    "\n",
    "## Regular expressions\n",
    "\n",
    "If we wanted to read out all of the stage directions for analysis, or just King Arthur's lines, doing so in base python string processing will be very difficult. Instead, we are going to use regular expressions. Regular expressions are a method for string manipulation that match patterns instead of bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "snippet = document.split(\"\\n\")[8]\n",
    "print(snippet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "re.search(r'coconuts', snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with `str.find`, we can search for plain text. But `re` also gives us the option for searching for patterns of bytes - like only alphabetic characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "re.search(r'[a-z]', snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we've told re to search for the first sequence of bytes that is only composed of lowercase letters between `a` and `z`. We could get the letters at the end of each sentence by including a bang at the end of the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "re.search(r'[a-z]!', snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two things happening here:\n",
    "\n",
    "1. `[` and `]` do not mean 'bracket'; they are special characters which mean 'any thing of this class'\n",
    "2. we've only matched one letter each\n",
    "\n",
    "Re is flexible about how you specify numbers - you can match none, some, a range, or all repetitions of a sequence or character class.\n",
    "\n",
    "character | meaning\n",
    "----------|--------\n",
    "`{x}`     | exactly x repetitions\n",
    "`{x,y}`   | between x and y repetitions\n",
    "`?`       | 0 or 1 repetition\n",
    "`*`       | 0 or many repetitions\n",
    "`+`       | 1 or many repetitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of the power of regular expressions are their special characters. Common ones that you'll see are:\n",
    "\n",
    "character | meaning\n",
    "----------|--------\n",
    "`.`       | match anything except a newline\n",
    "`^`       | match the start of a line\n",
    "`$`       | match the end of a line\n",
    "`\\s`      | matches any whitespace or newline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted to grab all of Arthur's speech without grabbing the name `ARTHUR` itself?\n",
    "\n",
    "If we wanted to do this using base string manipulation, we would need to do something like:\n",
    "\n",
    "```\n",
    "split the document into lines\n",
    "create a new list of just lines that start with ARTHUR\n",
    "create a newer list with ARTHUR removed from the front of each element\n",
    "```\n",
    "\n",
    "Regex gives us a way of doing this in one line, by using something called groups. Groups are pieces of a pattern that can be ignored, negated, or given names for later retrieval.\n",
    "\n",
    "character | meaning\n",
    "----------|--------\n",
    "`(x)`     | match x\n",
    "`(?:x)`   | match x but don't capture it\n",
    "`(?P<x>)` | match something and give it name x\n",
    "`(?=x)`   | match only if string is followed by x\n",
    "`(?!x)`   | match only if string is not followed by x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_arthur = re.compile(r'(?:ARTHUR: )(.+)')\n",
    "re.findall(p_arthur, document)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Because we are using `findall`, the regex engine is capturing and returning the normal groups, but not the non-capturing group. For complicated, multi-piece regular expressions, you may need to pull groups out separately. You can do this with names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = re.compile(r'(?P<name>[A-Z ]+)(?::)(?P<line>.+)')\n",
    "match = re.search(p, document)\n",
    "match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "match.group('name'), match.group('line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also list and count all the unique characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches = re.findall(p, document)\n",
    "chars = set([x[0] for x in matches])\n",
    "print(chars, len(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use this `set` to gather all dialogue and assign it to the correct character in a `dictionary`, so we can call it back whenever we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "char_dict = {}\n",
    "for n in chars:\n",
    "    char_dict[n] = re.findall(re.compile(r'(?:' + n + ': )(.+)'), document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "char_dict[\"ARTHUR\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's try a small challenge!\n",
    "\n",
    "To check that you've understood something about regular expressions, we're going to have you do a small test challenge. Partner up with the person next to you - we're going to do this as a pair coding exercise - and choose which computer you are going to use.\n",
    "\n",
    "Then, navigate to `challenges/04_text/` and read through challenge A. When you think you've completed it successfully, run `py.test test_A.py` .\n",
    "\n",
    "Enough with `regex`, let's move on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "Let's grab Arthur's speech from above, and see what we can learn about Arthur from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arthur = ' '.join(char_dict[\"ARTHUR\"])\n",
    "arthur[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our model for natural language, we're interested in words. The document is currently a continuous string of bytes, which isn't ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The practice of pulling apart a continuous string into units is called \"tokenizing\", and it creates \"tokens\". NLTK, the canonical library for NLP in Python, has a couple of implementations for tokenizing a string into words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "word_tokenize(snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at what happened to \"You're\". It's been separated into \"You\" and \"'re\", which keeps with the way contractions work in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens = word_tokenize(arthur)\n",
    "tokens[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can start asking questions like what are the most common words, and what words tend to occur together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(tokens), len(set(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see right away that Arthur is using the same words a whole bunch - on average, each unique word is used four times. This is typical of natural language. \n",
    "\n",
    "> Not necessarily the value, but that the number of unique words in any corpus increases much more slowly than the total number of words.\n",
    "\n",
    "> A corpus with 100M tokens, for example, probably only has 100,000 unique tokens in it.\n",
    "\n",
    "For more complicated metrics, it's easier to use NLTK's classes and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import collocations\n",
    "fd = collocations.FreqDist(tokens)\n",
    "fd.most_common()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove punctuation and stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rem_punc_stop(text_string):\n",
    "    \n",
    "    from string import punctuation\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    for char in punctuation:\n",
    "        text_string = text_string.replace(char, \"\")\n",
    "\n",
    "    toks = word_tokenize(text_string)\n",
    "    toks_reduced = [x for x in toks if x not in stopwords.words('english')]\n",
    "    \n",
    "    return toks_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now stopwords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens_reduced = rem_punc_stop(arthur)\n",
    "fd2 = collocations.FreqDist(tokens_reduced)\n",
    "fd2.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "measures = collocations.BigramAssocMeasures()\n",
    "c = collocations.BigramCollocationFinder.from_words(tokens_reduced)\n",
    "c.nbest(measures.pmi, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c.nbest(measures.likelihood_ratio, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that the collocation finder is pulling out some things that have face validity. When Arthur is talking about peasants, he calls them \"bloody\" more often than not. However, collocations like \"Brother Maynard\" and \"BLACK KNIGHT\" are less informative to us, because we know that they are proper names.\n",
    "\n",
    "If you were interested in collocations in particular, what step do you think you would have to take during the tokenizing process?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatizing\n",
    "\n",
    "This has gotten us as far identical tokens, but in language processing, it is often the case that the specific form of the word is not as important as the idea to which it refers. For example, if you are trying to identify the topic of a document, counting 'running', 'runs', 'ran', and 'run' as four separate words is not useful. Reducing words to their stems is a process called stemming.\n",
    "\n",
    "A popular stemming implementation is the Snowball Stemmer, which is based on the Porter Stemmer. It's algorithm looks at word forms and does things like drop final 's's, 'ed's, and 'ing's.\n",
    "\n",
    "Just like the tokenizers, we first have to create a stemmer object with the language we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "snowball = nltk.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can try stemming some words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snowball.stem('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snowball.stem('eats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snowball.stem('embarassed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snowball is a very fast algorithm, but it has a lot of edge cases. In some cases, words with the same stem are reduced to two different stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snowball.stem('cylinder'), snowball.stem('cylindrical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other cases, two different words are reduced to the same stem.\n",
    "\n",
    "> This is sometimes referred to as a 'collision'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "snowball.stem('vacation'), snowball.stem('vacate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more accurate approach is to use an English word bank like WordNet to call dictionary lookups on word forms, in a process called lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')\n",
    "wordnet = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wordnet.lemmatize('vacation'), wordnet.lemmatize('vacate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time for another small challenge!\n",
    "\n",
    "Switch computers for this one, so that you are using your partner's computer, and try your hand at challenge B!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment\n",
    "\n",
    "Frequently, we are interested in text to learn something about the person who is speaking. One of these things we've talked about already - linguistic diversity. A similar metric was used a couple of years ago to settle the question of who has the [largest vocabulary in Hip Hop](http://poly-graph.co/vocabulary.html).\n",
    "\n",
    "> Unsurprisingly, top spots go to Canibus, Aesop Rock, and the Wu Tang Clan. E-40 is also in the top 20, but mostly because he makes up a lot of words; as are OutKast, who print their lyrics with words slurred in the actual typography\n",
    "\n",
    "Another thing we can learn is about how the speaker is feeling, with a process called sentiment analysis. Before we start, be forewarned that this is not a robust method by any stretch of the imagination. Sentiment classifiers are often trained on product reviews, which limits their ecological validity.\n",
    "\n",
    "We're going to use TextBlob's built-in sentiment classifier, because it is super easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blob = TextBlob(arthur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net_pol = 0\n",
    "for sentence in blob.sentences:\n",
    "    pol = sentence.sentiment.polarity\n",
    "    print(pol, sentence)\n",
    "    net_pol += pol\n",
    "print()\n",
    "print(\"Net polarity of Arthur: \", net_pol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about we look at all characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "collected_stats = []\n",
    "for k in char_dict.keys():\n",
    "    blob = TextBlob(' '.join(char_dict[k]))\n",
    "    net_pol = 0\n",
    "    for sentence in blob.sentences:\n",
    "        pol = sentence.sentiment.polarity\n",
    "        net_pol += pol\n",
    "    collected_stats.append((k, net_pol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_stats = sorted(collected_stats, key=lambda x: x[1])\n",
    "for t in sorted_stats:\n",
    "    print(t[0], t[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "Another common NLP task is topic modeling. The math behind this is beyond the scope of this course, but the basic strategy is to represent each document as a one-dimensional array, where the indices correspond to integer ids of tokens in the document. Then, some measure of semantic similarity, like the cosine of the angle between unitized versions of the document vectors, is calculated. Finally, distinct topics are identified as leading certain groups of documents. The result is a list of `n` topics with the driving words for that topic, and a list of documents with their relation to each topic (how strongly a document fits that topic.\n",
    "\n",
    "Let's run a topic model on the characters of *Monty Python*.\n",
    "\n",
    "Luckily for us there is another Python library that takes care of the heavy lifting for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to separate the speeches and people, but keep it ordered so we index correctly when done. For the speeches, we'll need all speech as one string, then tokenized. We also need to remove punctuation and stop words so that Python can identify important words to documents. It seems we've gotten lucky again, we already wrote *rem_punc_stop* !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "people = []\n",
    "speeches = []\n",
    "for k,v in char_dict.items():\n",
    "    people.append(k)\n",
    "    new_string = ' '.join(v)\n",
    "    speeches.append(rem_punc_stop(new_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the dictionary of words used to create the matrices, and set thresholds for word frequencies within the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create a Gensim dictionary from the texts\n",
    "dictionary = corpora.Dictionary(speeches)\n",
    "\n",
    "#remove extremes (similar to the min/max df step used when creating the tf-idf matrix)\n",
    "#no_below is absolute # of docs, no_above is fraction of corpus\n",
    "dictionary.filter_extremes(no_below=2, no_above=.70)\n",
    "\n",
    "#convert the dictionary to a bag of words corpus for reference\n",
    "corpus = [dictionary.doc2bow(i) for i in speeches]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we set the parameters for the LDA topic modelling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#we run chunks of 15 books, and update after every 2 chunks, and make 10 passes\n",
    "lda = models.LdaModel(corpus, num_topics=6, \n",
    "                            update_every=2,\n",
    "                            id2word=dictionary, \n",
    "                            chunksize=15, \n",
    "                            passes=10)\n",
    "\n",
    "lda.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To match characters to their topics we just index the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "corpus_lda = lda[corpus_tfidf]\n",
    "for i, doc in enumerate(corpus_lda): # both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "    print(people[i],doc)\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice\n",
    "\n",
    "In the time remaining, pull up a dataset that you have, and that you'd like to work with in Python. The instructors will be around to help you apply what you've learned today to problems in your data that you are dealing with.\n",
    "\n",
    "If you don't have data of your own, you should practice with the test data we've given you here. For example, you could try to figure out:\n",
    "\n",
    "1. Is King Arthur happier than Sir Robin, based on his speech?\n",
    "2. Which character in Monty Python has the biggest vocabulary?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
