{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Classifying Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Tokenization (and basic bag-of-words features)\n",
    "\n",
    "Here is the code we went over at the start, to get started classifying documents by sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# Read in a list of document (wordlist, category) tuples, and shuffle\n",
    "docs_tuples = [(movie_reviews.words(fileid), category)\n",
    "               for category in movie_reviews.categories()\n",
    "               for fileid in movie_reviews.fileids(category)[:200]]\n",
    "random.shuffle(docs_tuples)\n",
    "\n",
    "# Create a list of the most frequent words in the entire corpus\n",
    "movie_words = [word.lower() for (wordlist, cat) in docs_tuples for word in wordlist]\n",
    "all_wordfreqs = nltk.FreqDist(movie_words)\n",
    "top_wordfreqs = all_wordfreqs.most_common()[:1000]\n",
    "feature_words = [x[0] for x in top_wordfreqs]\n",
    "\n",
    "# Define a function to extract features of the form containts(word) for each document\n",
    "def document_features(doc_toks):\n",
    "    document_words = set(doc_toks)\n",
    "    features = {}\n",
    "    for word in feature_words:\n",
    "        features['contains({})'.format(word)] = 1 if word in document_words else 0\n",
    "    return features\n",
    "\n",
    "# Create feature sets of document (features, category) tuples\n",
    "featuresets = [(document_features(wordlist), cat) for (wordlist, cat) in docs_tuples]\n",
    "\n",
    "# Separate train and test sets, train the classifier, print accuracy and best features\n",
    "train_set, test_set = featuresets[:-100], featuresets[-100:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We left the first part of the code the same as above, but created a new list of most common adjectives as our feature words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a list of the most frequent adjectives in the entire corpus\n",
    "from nltk import FreqDist\n",
    "\n",
    "movie_tokstags = nltk.pos_tag(movie_words)\n",
    "movie_adjs = [tok for (tok,tag) in movie_tokstags if re.match('JJ', tag)]\n",
    "all_adjfreqs = FreqDist(movie_adjs)\n",
    "top_adjfreqs = all_adjfreqs.most_common()[:1000]\n",
    "feature_words = [x[0] for x in top_adjfreqs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we left the document_features() function and remaining code the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define a function to extract features of the form containts(word) for each document\n",
    "def document_features(doc_toks):\n",
    "    document_words = set(doc_toks)\n",
    "    features = {}\n",
    "    for word in feature_words:\n",
    "        features['contains({})'.format(word)] = 1 if word in document_words else 0\n",
    "    return features\n",
    "\n",
    "# Create feature sets of document (features, category) tuples\n",
    "featuresets = [(document_features(wordlist), cat) for (wordlist, cat) in docs_tuples]\n",
    "\n",
    "# Separate train and test sets, train the classifier, print accuracy and best features\n",
    "train_set, test_set = featuresets[:-100], featuresets[-100:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Phrase Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we created a new list of most common noun phrases, and also modified the line where we use the document_features() function to extract the noun phrase features from each document's noun phrase list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a list of the most frequent noun phrases in the entire corpus\n",
    "from nltk import RegexpParser\n",
    "\n",
    "grammar = \"NP: {<JJ><NN.*>}\"\n",
    "cp = RegexpParser(grammar)\n",
    "\n",
    "def extract_nps(wordlist):\n",
    "    wordlist_tagged = nltk.pos_tag(wordlist)\n",
    "    wordlist_chunked = cp.parse(wordlist_tagged)\n",
    "    nps = []\n",
    "    for node in wordlist_chunked:\n",
    "        if type(node)==nltk.tree.Tree and node.label()=='NP':\n",
    "            phrase = [tok for (tok, tag) in node.leaves()]\n",
    "            nps.append(' '.join(phrase))\n",
    "    return nps\n",
    "\n",
    "docs_tuples_nps = [(extract_nps(wordlist), cat) for (wordlist, cat) in docs_tuples]\n",
    "\n",
    "movie_nps = [np for (nplist, cat) in docs_tuples_nps for np in nplist]\n",
    "all_npfreqs = FreqDist(movie_nps)\n",
    "top_npfreqs = all_npfreqs.most_common()[:1000]\n",
    "feature_nps = [x[0] for x in top_npfreqs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create feature sets of document (features, category) tuples\n",
    "featuresets = [(document_features(nplist), cat) for (nplist, cat) in docs_tuples_nps]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We left the last part of the code the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Separate train and test sets, train the classifier, print accuracy and best features\n",
    "train_set, test_set = featuresets[:-100], featuresets[-100:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This actually doesn't do that well. One reason is that we're using more complex sequences of words as our noun phrase features, each of which is going to appear far less frequently across documents. We might need to increase the number of noun phrases we use, or limit the pattern we're looking for to a single adjective followed by a single noun (leaving out articles, etc). But it might also be the case that adjectives are really the best features to use for sentiment classification in the domain of movie reviews, and the version of this task we did using POS tags was the right way to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Information Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Tokenization (and basic keyword search)\n",
    "\n",
    "Here is the code we went over at the start, to initially extract election-related sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "# Read in all news docs as a list of sentences, each sentence a list of tokens\n",
    "news_docs = [brown.sents(fileid) for fileid in brown.fileids(categories='news')]\n",
    "\n",
    "# Create regular expression to search for election-related words\n",
    "elect_regexp = 'elect|vote'\n",
    "\n",
    "# Loop through documents and extract each sentence containing an election-related word\n",
    "elect_sents = []\n",
    "for doc in news_docs:\n",
    "    for sent in doc:\n",
    "        for tok in sent:\n",
    "            if re.match(elect_regexp, tok):\n",
    "                elect_sents.append(sent)\n",
    "                break # Break out of last for loop, so we only add the sentence once\n",
    "            \n",
    "len(elect_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using POS Tagging\n",
    "\n",
    "We used the election-related sentences we identified in the first step (so we don't waste time tagging irrelevant text). Then we looped through each sentence, ran the POS tagger, and extracted all the nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract nouns from election-related sentences\n",
    "elect_nouns = []\n",
    "for sent in elect_sents:\n",
    "    sent_tagged = nltk.pos_tag(sent)\n",
    "    for (tok, tag) in sent_tagged:\n",
    "        if re.match('N', tag):\n",
    "            elect_nouns.append(tok)\n",
    "\n",
    "print(len(elect_nouns))\n",
    "print(elect_nouns[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add a check to see if the sentence has a token matching the election regexp that's tagged as a verb, once we've POS-tagged the sentence, and only add the sentence's nouns if the sentence passes this more specific test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract nouns if the sentence contains an election-related verb\n",
    "elect_nouns = []\n",
    "for sent in elect_sents:\n",
    "    sent_nouns = []\n",
    "    contains_elect_verb = False\n",
    "    sent_tagged = nltk.pos_tag(sent)\n",
    "    for (tok, tag) in sent_tagged:\n",
    "        if re.match('V', tag) and re.match(elect_regexp, tok):\n",
    "            contains_elect_verb = True\n",
    "        elif re.match('N', tag):\n",
    "            sent_nouns.append(tok)\n",
    "    if contains_elect_verb:\n",
    "        elect_nouns.extend(sent_nouns)\n",
    "\n",
    "print(len(elect_nouns))\n",
    "print(elect_nouns[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Phrase Chunking and NER Tagging\n",
    "\n",
    "Next we used the NLTK NER tagger (which chunks a sentence into named entity noun phrases, labeled by entity category), to extract named entities for either people or organizations mentioned in election-related sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "elect_entities = {'ORGANIZATION':[], 'PERSON':[]}\n",
    "for sent in elect_sents:\n",
    "    sent_tagged = nltk.pos_tag(sent)\n",
    "    sent_nes = nltk.ne_chunk(sent_tagged)\n",
    "    for node in sent_nes:\n",
    "        if type(node)==nltk.tree.Tree and node.label() in elect_entities:\n",
    "                phrase = [tok for (tok, tag) in node.leaves()]\n",
    "                elect_entities[node.label()].append(' '.join(phrase))\n",
    "\n",
    "for key, value in elect_entities.items():\n",
    "    print(key, value, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also extracted noun phrases if they appeared right before or after an election-related word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NN.*>+}\"\n",
    "cp = RegexpParser(grammar)\n",
    "\n",
    "entities_before = []\n",
    "entities_after = []\n",
    "\n",
    "for sent in elect_sents:\n",
    "    sent_tokstags = nltk.pos_tag(sent)\n",
    "    sent_chunks = cp.parse(sent_tokstags)\n",
    "    for n in range(len(sent_chunks)):\n",
    "        node = sent_chunks[n]\n",
    "        if type(node)!=nltk.tree.Tree and re.match(elect_regexp, node[0]):\n",
    "            if n > 0:\n",
    "                node_prev = sent_chunks[n-1]\n",
    "                if type(node_prev)==nltk.tree.Tree:\n",
    "                    phrase = ' '.join([tok for (tok, tag) in node_prev.leaves()])\n",
    "                    entities_before.append(phrase)\n",
    "            if n < len(sent_chunks)-1:\n",
    "                node_after = sent_chunks[n+1]\n",
    "                if type(node_after)==nltk.tree.Tree:\n",
    "                    phrase = ' '.join([tok for (tok, tag) in node_after.leaves()])\n",
    "                    entities_after.append(phrase)\n",
    "                \n",
    "print('BEFORE:', entities_before)\n",
    "print('AFTER:', entities_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "\n",
    "os.environ['STANFORD_PARSER'] = '/Users/natalieahn/Documents/SourceCode/stanford-parser-full-2015-12-09'\n",
    "os.environ['STANFORD_MODELS'] = '/Users/natalieahn/Documents/SourceCode/stanford-parser-full-2015-12-09'\n",
    "\n",
    "dependency_parser = StanfordDependencyParser(model_path=\"edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz\")\n",
    "sents_parsed = dependency_parser.parse_sents(elect_sents)\n",
    "sents_parseobjs = [obj for sent in sents_parsed for obj in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "elect_winners = []\n",
    "\n",
    "for sent_parseobj in sents_parseobjs:\n",
    "    sent_triples = sent_parseobj.triples()\n",
    "    for triple in sent_triples:\n",
    "        # Insert your code here\n",
    "        if re.match('win|won|defeat|gain|secure|achieve|got', triple[0][0]):\n",
    "            if re.match('nsubj', triple[1]):\n",
    "                elect_winners.append(triple[2][0])\n",
    "        elif re.match('elect|vote|choose|pick', triple[0][0]):\n",
    "            if re.match('dobj', triple[1]):\n",
    "                elect_winners.append(triple[2][0])\n",
    "\n",
    "print(elect_winners)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
